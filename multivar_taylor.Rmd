---
title: "Multivariate Analysis"
author: "Taylor Bonar & Satvik Ajmera"
output: html_document
---

```{r}
library(tswge)
library(arrow)
library(lubridate)
library(tidyverse)

train = read_parquet("data/btc-train.parquet")
test.short = read_parquet("data/btc-test-short.parquet")
test.long = read_parquet("data/btc-test-long.parquet")

# Reduce to a week of training data due to issues with library/performance
train = subset(train, timestamp > as.POSIXct("2021-05-24 23:59:00", tz="GMT"))

names(train)[1] <- "unix_time"
names(test.short)[1] <- "unix_time"
names(test.long)[1] <- "unix_time"
```

```{r}
# Clean into human-readable dates
train <- train %>% mutate(timestamp = as_datetime(unix_time))
test.short <- test.short %>% mutate(timestamp = as_datetime(unix_time))
test.long <- test.long %>% mutate(timestamp = as_datetime(unix_time))
```

# EDA

We are interested in identifying variables that impact BTC's Open price (Y) (variable Open in train)

We will avoid using variables such as Close, Low, & High to avoid data leakage and redundancy.
* Close is what we are essentially using with a lag 1 in an AR previously.

This leaves us with remaining variables to forecast the Open price for BTC:
* Count (X_1) -- the number of trades taking place this minute
* Volume (X_2) -- The number of cryptoasset units traded during the minute
* VWAP (X_3) -- The volume weighted average price for the minute
* Target (X_4) -- 15 minute residualized returns

Let's plot the data

```{r}
ggplot(train, aes(x=timestamp, y=Open)) +
  geom_line() +
  xlab("Timestamp") +
  ylab("BTC Open Price") +
  ggtitle("BTC Open Price over Time")
```
In our response variable, Open, we can observe a wandering behavior of the BTC opening price over the last week of May in 2021.

```{r}
ggplot(train, aes(x=timestamp, y=VWAP)) +
  geom_line() +
  xlab("Timestamp") +
  ylab("Volume Weighted Average Price") +
  ggtitle("BTC VWAP over Time")
```
VWAP may be serially correlated to Open as they have similar structure. This may provide data leakage and may need to be removed.


```{r}
ggplot(train, aes(x=timestamp, y=Count)) +
  geom_line() +
  xlab("Timestamp") +
  ylab("BTC Trade Count")  +
  ggtitle("BTC Count over Time")
```

```{r}
ggplot(train, aes(x=timestamp, y=Volume)) +
  geom_line() +
  xlab("Timestamp") +
  ylab("Cryptoassets Units Traded") +
  ggtitle("BTC Volume over Time")
```
The Volume and Count may be serially correlated structurally

```{r}
ggplot(train, aes(x=timestamp, y=Target)) +
  geom_line() +
  xlab("Timestamp") +
  ylab("Value of 15 Minute Residualized Return") +
  ggtitle("BTC Residualized Returns over Time")
```
The Residualized Return (Target) has a higher frequency as we already knew. This really is a moving average pass-through of the data that we discovered in our EDA. However, it does look serially correlated with some peaks in the data, but structurally is different.

For now we'll remove the VWAP variable due to data leakage, and use the remaining 3 variables.



# Multiple Regression with Correlated Errors

Step 1: Perform a regression analysis and model the residuals

We create a linear regression utilizing 3 additional variables: Count, Volume, and 15 Minutes Residual Returns (Target)
```{r}
# m = 3, n = 10080
ksfit = lm(data=train, Open~Count+Volume+Target)
```

Get AR(p) model identification using AIC for our Z_t 
```{r}
phi = aic.wge(ksfit$residuals, p=0:10, q=0) # AIC picks p=10
# residual assumption: uncorrelated and normally distributed; challenge that assumption
phi
```

Step 2: Use function ARIMA to perform the MLE analysis
* estimates the coefficients in the multiple regression while simultaneously modelling Z_t as an AR(phi$p)
```{r}
fit = arima(train$Open, order=c(phi$p, 0, 0), xreg=cbind(train$Count, train$Volume, train$Target))
fit
```
Note: fit$coef contains the AR coefficients, the constant, and the coefficients on Count, Volume, and Target

fit should produce "dummy" output w/ a phi$p=10

We can see our 10 phis (ar1-ar10), intercept, and our coefficients of our bound explanatory variables.
Note: it is a parameter estimate table; we'll use the standard error (s.e.) to determine what variables are significant or not
* arima function doesn't give p-values, but if the absolute value of the coefficient is over two times the SE, this is evidence at the .05 level that the variable is useful! (i.e., z or t statistic value > 2*s.e. == evidence of the alpha 0.05 level is useful)

Is Count a useful variable? **Yes!**
|-0.0036| > 2 * 0.0007
0.0036 > 0.0014

Is Volume a useful variable? **Yes!**
|0.0451| > 2 * 0.0100
0.0451 > 0.02 

Is Target a useful variable? **No**
|-1038.1181| > 2 * 693.2666
1018.627 < 1,386.53

**Multiple Regression Equation:**  
Open = 37,080.6242 - 0.0036(Count) + 0.0454(Volume) - 1018.627(Target)

Initial model residuals are in fit$residuals, and they should be white
* Can be checked with residual plots and/or Ljung-Box Test.
* Compare competing models with AIC/AICC/BIC etc.
```{r}
plot(fit$residuals)
acf(fit$residuals)
ltest = ljung.wge(fit$residuals)
ltest$pval
```
It looks like for the ACF, we have 8 small lags outside of our limits, and on our LJung-Box test, our p-value is 0.296. We fail to reject the null hypothesis.

These results are not enlightening or encouraging. So let's look into the time trend.
```{r}
# m = 4, n = 132,477
t = 1:length(train$Open)
ksfit_time = lm(data=train, Open~unix_time+Count+Volume+Target)
phi_time = aic.wge(ksfit_time$residuals, p=0:10, q=0)  # AIC picks p=10
fit_train = arima(train$Open, order=c(phi$p, 0, 0), xreg=cbind(t, train$Count, train$Volume, train$Target))
fit_train
```

**Multiple Regression Equation:**  
Open = 39,160.291 - 0.4126(minutes) - 0.0036(Count) + 0.0451(Volume) - 1,029.5847(Target)

Is the time trend a useful variable? **Yes!**
|-0.4126| > 2 * 0.1612
0.4126 > 0.3224

Is Count a useful variable? **Yes!**
|-0.0036| > 2 * 0.0006
0.0036 > 0.0012

Is Volume a useful variable? **Yes!**
|0.0451| > 2 * 0.0100
0.0451 > 0.02 

Is Target a useful variable? **No**
|-1028.5847| > 2 * 693.2666
1028.5847 < 1,386.925

```{r}
plot(fit_train$residuals)
acf(fit_train$residuals)
ltest2 = ljung.wge(fit_train$residuals)
ltest2$pval
```

# Lagged Variables

BTC's Open price didn't seem to have much significance with our residualized returns, Target, but what if it had an lagged effect?

Let's explore potential lagged variables

```{r}
ccf(train$Open, train$Target)
```
Using the ccf function, we can compute and check the estimates of our cross-correlations for Open and Target. We can observe that at lag 2, there appears to be a strong cross-correlation with the BTC Open price. Let's create a lagged 2 of Target and see how it affects our model.


```{r}
target_lagged2 = dplyr::lag(train$Target, 2)
train$Target_Lagged2 = target_lagged2

ksfit.lagged = lm(data=train, Open~Count+Volume+Target_Lagged2)
phi.lagged = aic.wge(ksfit_time$residuals, p=0:10, q=0)  # AIC picks p=10
fit.lagged = arima(train$Open, order=c(phi.lagged$p, 0, 0), xreg=cbind(train$Count, train$Volume, train$Target_Lagged2))
fit.lagged
```
Here we can see the Target of lag 2 variable is now highly significant to the model.

Is Target, with a lag of 2, a useful variable? **Yes!**
|-19215.2742| > 2 * 635.5321
19215.2742 > 1,271.064

Lastly, let's examine our time trend with this lagged variable.

```{r}
ksfit.lagged.time = lm(data=train, Open~Count+Volume+Target_Lagged2)
phi.lagged.time = aic.wge(ksfit_time$residuals, p=0:10, q=0)  # AIC picks p=10
fit.lagged.time = arima(train$Open, order=c(phi.lagged.time$p, 0, 0), xreg=cbind(t, train$Count, train$Volume, train$Target_Lagged2))
fit.lagged.time
```


# VAR
Let's look at lagged values of our explanatory variables and how they might interact with the BTC Open price. We want to predict the Open time series with its own lagged values and any lagged values of our explanatory variables: Count, Volume, Target.

Open Univariate Forecasts
```{r}
open.best.p = aic.wge(train$Open, p=0:20, q=0:0) # AIC picks p = 3
open.est = est.ar.wge(train$Open, open.best.p$p) # Estimate the parameters
fore.arma.wge(train$Open, phi=open.est$phi, n.ahead=1440, lastn=FALSE, limits=FALSE)
```

Count Univariate Forecasts
```{r}
count.best.p = aic.wge(train$Count, p=0:20, q=0:0) # AIC picks p = 18
count.est = est.ar.wge(train$Count, count.best.p$p) # Estimate the parameters
fore.arma.wge(train$Count, phi=count.est$phi, n.ahead=1440, lastn=FALSE, limits=FALSE)
```


Volume Univariate Forecasts
```{r}
volume.best.p = aic.wge(train$Volume, p=0:20, q=0:0) # AIC picks p = 18
volume.est = est.ar.wge(train$Volume, volume.best.p$p)
fore.arma.wge(train$Count, phi=volume.est$phi, n.ahead=1440, lastn=FALSE, limits=FALSE)
```

15 min Residualized Returns Univariate Forecasts
```{r}
target.best.p = aic.wge(train$Target, p=0:20, q=0:0) # AIC picks p = 19
target.est = est.ar.wge(train$Target, volume.best.p$p)
fore.arma.wge(train$Target, phi=target.est$phi, n.ahead=1440, lastn=FALSE, limits=FALSE)
```

Modelling with VARSelect as Multivariate

We know from our Multiple Regression with Correlated Errors that our Target tends to be the most significant variable when predicting the Open price of BTC. So let's compare against a Vector Auto Regression.
```{r}
library(vars)

X = cbind(Open=train$Open, Count=train$Count, Volume=train$Volume, Target=train$Target)
VARselect(X, lag.max = 20, season = NULL, exogen = NULL)
# VARselect: AIC/HQ/SC/FPE=18
```

```{r}
lsfit = VAR(X, p=18, type='const')
lsfit
```
Short-Term Forecasting using VAR
```{r}
var.preds.short = predict(lsfit, n.ahead=1440)
var.preds.short
```

Long-Term Forecasting using VAR
```{r}
var.preds.long = predict(lsfit, n.ahead=10080)
var.preds.long
```


```{r}
# ggplot(data = test.short, aes(unix_time, y=Open)) + geom_line()
# ggplot(data = var.preds.short, aes(var.preds.short$fcst$Open)) + geom_line(aes(colour="red"))
plot(seq(1,1439,1), test.short$Open, type = "l")
points(seq(1,1439,1), var.preds.short$fcst$Open[1:1439], pch = 5, col="red")
```

```{r}
# ggplot(data = test.short, aes(unix_time, y=Open)) + geom_line()
# ggplot(data = var.preds.short, aes(var.preds.short$fcst$Open)) + geom_line(aes(colour="red"))
plot(seq(1,10079,1), test.long$Open, type = "l")
points(seq(1,10079,1), var.preds.long$fcst$Open[1:10079], pch = 5, col="red")
```

With time trend:
```{r}
X.time = cbind(Open=train$Open, t=t, Count=train$Count, Volume=train$Volume, Target=train$Target)
VARselect(X, lag.max = 20, season = NULL, exogen = NULL)
# VARselect: AIC/HQ/SC/FPE=18
lsfit.time = VAR(X.time, p=18, type='const')
lsfit.time
```

```{r}
var.time.preds.short = predict(lsfit.time, n.ahead=1440)
var.time.preds.short
```


```{r}
library(RColorBrewer)

fanchart(var.preds.short, colors=brewer.pal(n=8, name = "Blues"))
```


Long-Term Forecasting using VAR
```{r}
var.time.preds.long = predict(lsfit.time, n.ahead=10080)
var.time.preds.long
```



```{r}
fanchart(var.preds.long, colors=brewer.pal(n=8, name = "Blues"))
```
# Neural Network/MLP
```{r}
library(nnfor)

set.seed(42)

#Subset training data due to issues w/ library w/ large datasets
mlp.train2 = subset(train, timestamp > as.POSIXct("2021-05-24 23:59:00", tz="GMT"))

tsdat2 = ts(data = mlp.train2$Open)

fit.mlp2 = mlp(tsdat2)
fit.mlp2
```

```{r}
plot(fit.mlp2)
```

```{r}
fc.short.period = 1439
fc.short = forecast(fit.mlp2, h=fc.short.period)
```

```{r}
plot(fc.short)
```

```{r}
mlp.short.ASE = mean((test.short$Open - fc.short$f)^2)
mlp.short.ASE
# ASE = 3317252
sqrt(mlp.short.ASE)
# Root ASE = 1821.332
```

```{r}
fc.long.period = 10080
fc.long = forecast(fit.mlp2, h=fc.long.period)
```

```{r}
plot(fc.long)
```

```{r}
mlp.long.ASE = mean((test.long$Open[1:10077] - fc.long$f)^2)
mlp.long.ASE
# ASE = 1942979
sqrt(mlp.long.ASE)
# Root ASE = 1393.908
```

